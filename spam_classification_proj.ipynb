{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rammohith/AD-NLP/blob/main/spam_classification_proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpUiC1t53Tle",
        "outputId": "66c7c1a1-759b-482f-fa6b-416091872ea1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/abdallahwagih/spam-emails\n",
            "License(s): apache-2.0\n",
            "spam-emails.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  spam-emails.zip\n",
            "replace spam.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize  # For tokenizing text into words and sentences\n",
        "\n",
        "# Importing stopwords from NLTK to remove common words that add little value\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Downloading required NLTK datasets\n",
        "nltk.download('punkt')  # Downloads tokenizer models for sentence and word tokenization\n",
        "nltk.download('punkt_tab')  # Optional: Additional support for tokenization\n",
        "nltk.download('stopwords')  # Downloads predefined stopword lists for various languages\n",
        "\n",
        "# Downloading the dataset from Kaggle using Kaggle CLI\n",
        "# Here, we download a dataset containing spam emails\n",
        "!kaggle datasets download -d abdallahwagih/spam-emails  # Downloads the spam emails dataset\n",
        "!unzip spam-emails.zip  # Extracts the downloaded dataset\n",
        "\n",
        "# Importing pandas for data manipulation\n",
        "import pandas as pd\n",
        "\n",
        "# Loading the dataset into a pandas DataFrame\n",
        "# The CSV file contains columns like 'Message' (email text) and 'Label' (spam/not spam indicator)\n",
        "df = pd.read_csv(\"spam.csv\")\n",
        "\n",
        "import re  # Regular expressions module for text cleaning\n",
        "\n",
        "cleaned = []  # List to store cleaned text\n",
        "for text in df['Message']:  # Iterating over each message in the 'Message' column\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)  # Removing punctuation and special characters\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # Replacing multiple spaces with a single space\n",
        "    cleaned_data = cleaned_text.strip()  # Removing leading and trailing spaces\n",
        "    cleaned.append(cleaned_data)  # Appending the cleaned text to the list\n",
        "\n",
        "# Tokenizing the cleaned text into words\n",
        "# Each cleaned text is split into a list of individual words for further analysis\n",
        "tokens = [word_tokenize(x) for x in cleaned]\n",
        "\n",
        "# Removing stopwords from the tokenized words\n",
        "# Stopwords are common words (e.g., \"the\", \"is\") that do not contribute much to analysis\n",
        "stop = set(stopwords.words('english'))  # Fetching the list of English stopwords\n",
        "stop_token = []  # List to store tokens after removing stopwords\n",
        "for k in range(len(df['Message'])):  # Iterating through tokenized text\n",
        "    p = [i for i in tokens[k] if i not in stop]  # Filtering out stopwords\n",
        "    stop_token.append(p)  # Adding filtered tokens to the list\n",
        "\n",
        "# Applying stemming to reduce words to their root form\n",
        "# This helps group similar words (e.g., \"running\", \"runner\" -> \"run\")\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()  # Initializing the Porter Stemmer\n",
        "stemedata = []  # List to store stemmed data\n",
        "for message in stop_token:  # Iterating over tokens after stopword removal\n",
        "    st = [ps.stem(word) for word in message]  # Applying stemming to each word\n",
        "    stemedata.append(st)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9An4_h28q2GN"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXhDh7G05HXa"
      },
      "outputs": [],
      "source": [
        "#apply pos_tags on stop token\n",
        "\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTGgpdNf5qK0"
      },
      "outputs": [],
      "source": [
        "pos_tokens = [pos_tag(i) for i in stop_token]\n",
        "pos_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUhncAwL5xqe"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeESz3ES62XG"
      },
      "outputs": [],
      "source": [
        "lm = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtYYyKtj646e"
      },
      "outputs": [],
      "source": [
        "lm.lemmatize('running','v')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GD5QPanp7Ej1"
      },
      "outputs": [],
      "source": [
        "pos_tokens[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZk6vSJ-7Uyx"
      },
      "outputs": [],
      "source": [
        "#function to convert nltks pos tags to wordnet pos tags\n",
        "from nltk.corpus import wordnet\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ  # Adjective\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB  # Verb\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN  # Noun\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV  # Adverb\n",
        "    else:\n",
        "        return wordnet.NOUN  # Default to noun if"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QO-CEt759TwH"
      },
      "outputs": [],
      "source": [
        "get_wordnet_pos(pos_tokens[0][0][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZAURkqv8iYG"
      },
      "outputs": [],
      "source": [
        "pos_tokens[0][0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFjKDfFJ9B98"
      },
      "outputs": [],
      "source": [
        "pos_tokens[0][0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPAmQbp_9qYD"
      },
      "outputs": [],
      "source": [
        "lm.lemmatize(pos_tokens[0][0][0],get_wordnet_pos(pos_tokens[0][0][1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQAWF71r_vce"
      },
      "outputs": [],
      "source": [
        "lemmed_data = []\n",
        "for x in range (len(pos_tokens)):\n",
        "    lem = [lm.lemmatize(pos_tokens[x][y][0],get_wordnet_pos(pos_tokens[x][y][1])) for y in range(len(pos_tokens[x]))]\n",
        "    lemmed_data.append(lem)\n",
        "lemmed_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsLmwhTfe27w"
      },
      "outputs": [],
      "source": [
        "#applying count vectorization to convert text data into numerical format using bag of words\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dlrk0JSdlQi0"
      },
      "outputs": [],
      "source": [
        "sen_stem = [' '.join(i) for i in stemedata]\n",
        "sen_stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDTtGwp7ip-M"
      },
      "outputs": [],
      "source": [
        "X_vec = cv.fit_transform(sen_stem).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHUz1zBFl0dU"
      },
      "outputs": [],
      "source": [
        "X_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zachSn8ri16g"
      },
      "outputs": [],
      "source": [
        "#importing multinomial nb model\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "mb = MultinomialNB()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mb.fit(X_vec,y)"
      ],
      "metadata": {
        "id": "K1XELxaxOvlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mb.predict([X_vec[0]])"
      ],
      "metadata": {
        "id": "pjS7MhYbO3Fg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT5JYrth20cPMSBdUthyud",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}